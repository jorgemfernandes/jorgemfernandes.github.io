---
---

@InProceedings{pmlr-v108-cao20a,
abbr={AISTATS},
title = {Better Long-Range Dependency By Bootstrapping A Mutual Information Regularizer},
author = {Cao, Yanshuai and Xu, Peng},
booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
pages = {3991--4001},
year = {2020},
editor = {Silvia Chiappa and Roberto Calandra}, volume = {108},
series = {Proceedings of Machine Learning Research},
address = {Online},
month = {26--28 Aug},
publisher = {PMLR},
pdf = {http://proceedings.mlr.press/v108/cao20a/cao20a.pdf},
html = {http://proceedings.mlr.press/v108/cao20a.html},
abstract = {In this work, we develop a novel regularizer to improve the learning of long-range dependency of sequence data. Applied on language modelling, our regularizer expresses the inductive bias that sequence variables should have high mutual information even though the model might not see abundant observations for complex long-range dependency. We show how the “next sentence prediction (classification)" heuristic can be derived in a principled way from our mutual information estimation framework, and be further extended to maximize the mutual information of sequence variables. The proposed approach not only is effective at increasing the mutual information of segments under the learned model but more importantly, leads to a higher likelihood on holdout data, and improved generation quality. Code is releasedat https://github.com/BorealisAI/BMI.},
arxiv={1905.11978},
code={https://github.com/BorealisAI/BMI},
selected={true},
equal_last = {Cao, Xu},
}

@InProceedings{pmlr-v119-huang20c,
abbr={ICML},
title = {Evaluating Lossy Compression Rates of Deep Generative Models}, author = {Huang, Sicong and Makhzani, Alireza and Cao, Yanshuai and Grosse, Roger},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
pages = {4444--4454}, year = {2020},
editor = {Hal Daumé III and Aarti Singh},
volume = {119},
series = {Proceedings of Machine Learning Research},
address = {Virtual},
month = {13--18 Jul},
publisher = {PMLR},
pdf = {http://proceedings.mlr.press/v119/huang20c/huang20c.pdf},
html = {http://proceedings.mlr.press/v119/huang20c.html},
abstract = {The field of deep generative modeling has succeeded in producing astonishingly realistic-seeming images and audio, but quantitative evaluation remains a challenge. Log-likelihood is an appealing metric due to its grounding in statistics and information theory, but it can be challenging to estimate for implicit generative models, and scalar-valued metrics give an incomplete picture of a model’s quality. In this work, we propose to use rate distortion (RD) curves to evaluate and compare deep generative models. While estimating RD curves is seemingly even more computationally demanding than log-likelihood estimation, we show that we can approximate the entire RD curve using nearly the same computations as were previously used to achieve a single log-likelihood estimate. We evaluate lossy compression rates of VAEs, GANs, and adversarial autoencoders (AAEs) on the MNIST and CIFAR10 datasets. Measuring the entire RD curve gives a more complete picture than scalar-valued metrics, and we arrive at a number of insights not obtainable from log-likelihoods alone.},
arxiv={2008.06653},
code={https://github.com/huangsicong/rate_distortion},
selected={true},
equal_last = {Huang, Makhzani},
}

@InProceedings{pmlr-v119-xu20a,
abbr={ICML},
title = {On Variational Learning of Controllable Representations for Text without Supervision},
author = {Xu, Peng and Cheung, Jackie Chi Kit and Cao, Yanshuai},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
pages = {10534--10543},
year = {2020},
editor = {Hal Daumé III and Aarti Singh},
volume = {119},
series = {Proceedings of Machine Learning Research}, address = {Virtual}, month = {13--18 Jul}, publisher = {PMLR},
pdf = {http://proceedings.mlr.press/v119/xu20a/xu20a.pdf},
url = {http://proceedings.mlr.press/v119/xu20a.html},
abstract = {The variational autoencoder (VAE) can learn the manifold of natural images on certain datasets, as evidenced by meaningful interpolating or extrapolating in the continuous latent space. However, on discrete data such as text, it is unclear if unsupervised learning can discover similar latent space that allows controllable manipulation. In this work, we find that sequence VAEs trained on text fail to properly decode when the latent codes are manipulated, because the modified codes often land in holes or vacant regions in the aggregated posterior latent space, where the decoding network fails to generalize. Both as a validation of the explanation and as a fix to the problem, we propose to constrain the posterior mean to a learned probability simplex, and performs manipulation within this simplex. Our proposed method mitigates the latent vacancy problem and achieves the first success in unsupervised learning of controllable representations for text. Empirically, our method outperforms unsupervised baselines and strong supervised approaches on text style transfer, and is capable of performing more flexible fine-grained control over text generation than existing methods.},
html={http://proceedings.mlr.press/v119/xu20a.html},
arxiv={1905.11975},
code={https://github.com/BorealisAI/CP-VAE},
selected={true},
}

@article{deng2020variational,
abbr={Preprint},
  title={Variational Hyper RNN for Sequence Modeling},
  author={Deng, Ruizhi and Cao, Yanshuai and Chang, Bo and Sigal, Leonid and Mori, Greg and Brubaker, Marcus A},
  journal={arXiv preprint arXiv:2002.10501},
  year={2020},
  abstract={In this work, we propose a novel probabilistic sequence model that excels at capturing high variability in time series data, both across sequences and within an individual sequence. Our method uses temporal latent variables to capture information about the underlying data pattern and dynamically decodes the latent information into modifications of weights of the base decoder and recurrent model. The efficacy of the proposed method is demonstrated on a range of synthetic and real-world sequential data that exhibit large scale variations, regime shifts, and complex dynamics.},
  arxiv={2002.10501}
}

@article{long2019preventing,
abbr={Preprint},
  title={Preventing Posterior Collapse in Sequence VAEs with Pooling},
  author={Long, Teng and Cao, Yanshuai and Cheung, Jackie Chi Kit},
  journal={arXiv preprint arXiv:1911.03976},
  year={2019},
  abstract={Variational autoencoders (VAEs) hold great potential for modelling text, as they could in theory separate high-level semantic and syntactic properties from local regularities of natural language. Practically, however, VAEs with autoregressive decoders often suffer from posterior collapse, a phenomenon where the model learns to ignore the latent variables, causing the sequence VAE to degenerate into a language model. In this paper, we argue that posterior collapse is in part caused by the lack of dispersion in encoder features. We provide empirical evidence to verify this hypothesis, and propose a straightforward fix using pooling. This simple technique effectively prevents posterior collapse, allowing model to achieve significantly better data log-likelihood than standard sequence VAEs. Comparing to existing work, our proposed method is able to achieve comparable or superior performances while being more computationally efficient.},
  arxiv={1911.03976}
}

@inproceedings{xu-etal-2019-cross,
abbr={ACL},
    title = "A Cross-Domain Transferable Neural Coherence Model",
    author = "Xu, Peng  and
      Saghir, Hamidreza  and
      Kang, Jin Sung  and
      Long, Teng  and
      Bose, Avishek Joey  and
      Cao, Yanshuai  and
      Cheung, Jackie Chi Kit",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1067",
    doi = "10.18653/v1/P19-1067",
    pages = "678--687",
    abstract = {Coherence is an important aspect of text quality and is crucial for ensuring its readability. One important limitation of existing coherence models is that training on one domain does not easily generalize to unseen categories of text. Previous work advocates for generative models for cross-domain generalization, because for discriminative models, the space of incoherent sentence orderings to discriminate against during training is prohibitively large. In this work, we propose a local discriminative neural model with a much smaller negative sampling space that can efficiently learn against incorrect orderings. The proposed coherence model is simple in structure, yet it significantly outperforms previous state-of-art methods on a standard benchmark dataset on the Wall Street Journal corpus, as well as in multiple new challenging settings of transfer to unseen categories of discourse on Wikipedia articles.},
    arxiv={1905.11912},
    code={https://github.com/BorealisAI/cross_domain_coherence},
    selected={true},
}

@article{wen2018few,
abbr={Preprint},
  title={Few-shot self reminder to overcome catastrophic forgetting},
  author={Wen, Junfeng and Cao, Yanshuai and Huang, Ruitong},
  journal={NeurIPS 2018 Workshop on Continual Learning},
  year={2018},
  abstract={Deep neural networks are known to suffer the catastrophic forgetting problem, where they tend to forget the knowledge from the previous tasks when sequentially learning new tasks. Such failure hinders the application of deep learning based vision system in continual learning settings. In this work, we present a simple yet surprisingly effective way of preventing catastrophic forgetting. Our method, called Few-shot Self Reminder (FSR), regularizes the neural net from changing its learned behaviour by performing logit matching on selected samples kept in episodic memory from the old tasks. Surprisingly, this simplistic approach only requires to retrain a small amount of data in order to outperform previous methods in knowledge retention. We demonstrate the superiority of our method to the previous ones in two different continual learning settings on popular benchmarks, as well as a new continual learning problem where tasks are designed to be more dissimilar.},
  arxiv={1812.00543}
}

@article{Bose2018CompositionalHN,
abbr={Workshop},
  title={Compositional Hard Negatives for Visual Semantic Embeddings via an Adversary},
  author={A. Bose and Huan Ling and Yanshuai Cao},
  year={2018},
  journal={NeurIPS 2018 Workshop on ViGIL},
    html={https://nips2018vigil.github.io/static/papers/accepted/20.pdf},
  abstract={Learning high-quality representations for multi-modal data with a shared underlying meaning is a key building block for cross-modal information retrieval. Further, hard negative mining has been shown effective in forcing models to learn discriminaitve features for accurate retrieval. In this paper, we present a new technique for hard negative mining for learning visual-semantic embeddings, with an adversary that is learned in a min-max game with the cross-modal embedding model. The adversary exploits compositionality of images and texts and is able to compose harder negatives through novel combination of objects and regions across different images for a given caption. We find that our approach leads to higher scores across-the-board for all R@K based metrics over the previous state of the art.},
}

@Article{Cao2018Improving,
abbr={ICLR},
Title = {Improving GAN Training via Binarized Representation Entropy (BRE) Regularization},
Author = {Yanshuai Cao and Gavin Weiguang Ding and Kry Yik-Chau Lui and Ruitong Huang},
Journal = {International Conference on Learning Representations, {ICLR}},
Year = {2018},
html = {https://openreview.net/forum?id=BkLhaGZRW},
arxiv={1805.03644},
code={https://github.com/BorealisAI/bre-gan},
abstract={We propose a novel regularizer to improve the training of Generative Adversarial Networks (GANs). The motivation is that when the discriminator D spreads out its model capacity in the right way, the learning signals given to the generator G are more informative and diverse. These in turn help G to explore better and discover the real data manifold while avoiding large unstable jumps due to the erroneous extrapolation made by D. Our regularizer guides the rectifier discriminator D to better allocate its model capacity, by encouraging the binary activation patterns on selected internal layers of D to have a high joint entropy. Experimental results on both synthetic data and real datasets demonstrate improvements in stability and convergence speed of the GAN training, as well as higher sample quality. The approach also leads to higher classification accuracies in semi-supervised learning.},
selected={true},
}

@article{wang2018adversarial,
abbr={Preprint},
  title={Adversarial robustness of pruned neural networks},
  author={Wang, Luyu and Ding, Gavin Weiguang and Huang, Ruitong and Cao, Yanshuai and Lui, Yik Chau},
  year={2018},
  abstract={Deep neural network pruning forms a compressed network by discarding “unimportant” weights or filters. Standard evaluation metrics have shown their remarkable speedup and prediction accuracy in test time, but their adversarial robustness remains unexplored even though it is an important security feature in deployment. We study the robustness of pruned neural networks under adversarial
attacks. We discover that although pruned models maintain the original accuracy,
they are more vulnerable to such attacks. We further show that adversarial training improves the robustness of pruned networks. However, it is observed there
exist trade-offs among compression rate, accuracy and robustness in adversarially trained pruned neural networks. Our analysis suggests that we should pay
additional attention to robustness in neural network pruning rather than just maintaining the classification accuracy.}
}

@inproceedings{bose-etal-2018-adversarial,
abbr={ACL},
    title = "Adversarial Contrastive Estimation",
    author = "Bose, Avishek Joey  and
      Ling, Huan  and
      Cao, Yanshuai",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1094",
    doi = "10.18653/v1/P18-1094",
    pages = "1021--1032",
    arxiv = {1805.03642},
    html={https://www.aclweb.org/anthology/P18-1094/},
    abstract = "Learning by contrasting positive and negative samples is a general strategy adopted by many methods. Noise contrastive estimation (NCE) for word embeddings and translating embeddings for knowledge graphs are examples in NLP employing this approach. In this work, we view contrastive learning as an abstraction of all such methods and augment the negative sampler into a mixture distribution containing an adversarially learned sampler. The resulting adaptive sampler finds harder negative examples, which forces the main model to learn a better representation of the data. We evaluate our proposal on learning word embeddings, order embeddings and knowledge graph embeddings and observe both faster convergence and improved results on multiple metrics.",
    selected={true},
    equal_last = {Bose, Ling, Cao},
}

@phdthesis{Cao_Yanshuai_PhD_thesis,
abbr={PhD Thesis},
  author       = {Cao, Yanshuai}, 
  title        = {Scaling Gaussian Processes},
  school       = {University of Toronto},
  html = {https://tspace.library.utoronto.ca/handle/1807/89683},
  year         = 2018,
}


@article{lui2017implicit,
abbr={Workshop},
  title={Implicit manifold learning on generative adversarial networks},
  author={Lui, Kry Yik Chau and Cao, Yanshuai and Gazeau, Maxime and Zhang, Kelvin Shuangjian},
  year={2017},
  arxiv={1710.11260},
  Journal = {ICML 2017 Workshop on Implicit Models},
}

@article{CaoAST,
abbr={Workshop},
Title = {Automatic Selection of t-SNE Perplexity},
Author = {Yanshuai Cao and Luyu Wang},
Year = {2017},
Abstract = {t-Distributed Stochastic Neighbor Embedding (t-SNE) is one of the most widely used dimensionality reduction methods for data visualization, but it has a perplexity hyperparameter that requires manual selection. In practice, proper tuning of t-SNE perplexity requires users to understand the inner working of the method as well as to have hands-on experience. We propose a model selection objective for t-SNE perplexity that requires negligible extra computation beyond that of the t-SNE itself. We empirically validate that the perplexity settings found by our approach are consistent with preferences elicited from human experts across a number of datasets. The similarities of our approach to Bayesian information criteria (BIC) and minimum description length (MDL) are also analyzed.},
Journal = {ICML 2017 Workshop on AutoML},
Url = {http://arxiv.org/abs/1708.03229},
arxiv={1708.03229}
}

@inproceedings{SabourCFF15,
abbr={ICLR},
  title     = {Adversarial Manipulation of Deep Representations},
  author    = {Sabour, Sara and
               Cao, Yanshuai  and
               Faghri, Fartash and
               Fleet, David J. },  
  booktitle = {4th International Conference on Learning Representations, {ICLR}},             
  year      = {2016},
  url       = {http://arxiv.org/abs/1511.05122},
  arxiv = {1511.05122},
  abstract = {We show that the image representations in a deep neural network (DNN) can be
manipulated to mimic those of other natural images, with only minor, imperceptible perturbations to the original image. Previous methods for generating adversarial images focused on image perturbations designed to produce erroneous class labels. Here we instead concentrate on the internal layers of DNN representations, to produce a new class of adversarial images that differs qualitatively from others. While the adversary is perceptually similar to one image, its internal representation appears remarkably similar to a different image, from a different class and
bearing little if any apparent similarity to the input. Further, they appear generic
and consistent with the space of natural images. This phenomenon demonstrates
the possibility to trick a DNN to confound almost any image with any other chosen
image, and raises questions about DNN representations, as well as the properties
of natural images themselves.},
  code = {https://github.com/fartashf/under_convnet},
  equal_last = {Sabour, Cao},
}

@ARTICLE{tpami15_caoy,
abbr={TPAMI},
 author = {Cao, Yanshuai and Brubaker, Marcus A and Fleet, David J and Hertzmann, Aaron},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Efficient Optimization for Sparse Gaussian Process Regression}, 
  year={2015},
  volume={37},
  number={12},
  pages={2415-2427},
  doi={10.1109/TPAMI.2015.2424873},
  html={https://ieeexplore.ieee.org/document/7089279},
   code = {https://github.com/yanshuaicao/gp_cholqr},
   abstract={We propose an efficient optimization algorithm to select a subset of training data as the inducing set for sparse Gaussian process regression. Previous methods either use different objective functions for inducing set and hyperparameter selection, or else optimize the inducing set by gradient-based continuous optimization. The former approaches are harder to interpret and suboptimal, whereas the latter cannot be applied to discrete input domains or to kernel functions that are not differentiable with respect to the input. The algorithm proposed in this work estimates an inducing set and the hyperparameters using a single objective. It can be used to optimize either the marginal likelihood or a variational free energy. Space and time complexity are linear in training set size, and the algorithm can be applied to large regression problems on discrete or continuous domains. Empirical evaluation shows state-of-art performance in discrete cases, competitive prediction results as well as a favorable trade-off between training and test time in continuous cases.}
  }

@article{cao2015transductive,
abbr={Workshop},
  title={Transductive Log Opinion Pool of Gaussian Process Experts},
  author={Cao, Yanshuai and Fleet, David J},
  journal={NIPS2015 Workshop on Nonparametric Methods for Large Scale Representation Learning},
  year={2015},
  abstract={We introduce a framework for analyzing transductive combination of Gaussian
process (GP) experts, where independently trained GP experts are combined in a
way that depends on test point location, in order to scale GPs to big data. The
framework provides some theoretical justification for the generalized product of
GP experts (gPoE-GP) which was previously shown to work well in practice [2, 3]
but lacks theoretical basis. Based on the proposed framework, an improvement
over gPoE-GP is introduced and empirically validated.},
arxiv={1511.07551}
}


@article{cao2014generalized,
abbr={Workshop},
  title={Generalized Product of Experts for Automatic and Principled Fusion of Gaussian Process Predictions},
  author={Cao, Yanshuai and Fleet, David J},
  journal={Modern Nonparametrics 3: Automating the Learning Pipeline Workshop at NIPS},
  year={2014},
  abstract = {In this work, we propose a generalized product of experts (gPoE) framework for
combining the predictions of multiple probabilistic models. We identify four desirable properties that are important for scalability, expressiveness and robustness,
when learning and inferring with a combination of multiple models. Through
analysis and experiments, we show that gPoE of Gaussian processes (GP) have
these qualities, while no other existing combination schemes satisfy all of them
at the same time. The resulting GP-gPoE is highly scalable as individual GP experts can be independently learned in parallel; very expressive as the way experts
are combined depends on the input rather than fixed; the combined prediction is
still a valid probabilistic model with natural interpretation; and finally robust to
unreliable predictions from individual experts.},
  arxiv={1410.7827}
}

@inproceedings{NIPS2013_46922a08,
abbr={NeurIPS},
 author = {Cao, Yanshuai and Brubaker, Marcus A and Fleet, David J and Hertzmann, Aaron},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
 pages = {1097--1105},
 publisher = {Curran Associates, Inc.},
 title = {Efficient Optimization for Sparse Gaussian Process Regression},
 html = {https://papers.nips.cc/paper/2013/hash/46922a0880a8f11f8f69cbb52b1396be-Abstract.html},
 arxiv= {1310.6007},
 volume = {26},
 year = {2013},
 abstract = {We propose an efficient discrete optimization algorithm for selecting a subset of training data to induce sparsity for Gaussian process regression. The algorithm estimates this inducing set and the hyperparameters using a single objective, either the marginal likelihood or a variational free energy. The space and time complexity are linear in the training set size, and the algorithm can be applied to large regression problems on discrete or continuous domains. Empirical evaluation shows state-of-art performance in the discrete case and competitive results in the continuous case.},
 code = {https://github.com/yanshuaicao/gp_cholqr},
 supp = {https://papers.nips.cc/paper/2013/file/46922a0880a8f11f8f69cbb52b1396be-Supplemental.zip}
}


